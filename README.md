# MVTec Anomaly Detection Evaluation Pipeline

## Overview

This project provides a fully automated pipeline for evaluating anomaly detection models on the MVTec Anomaly Detection (AD) dataset. It is designed to be configurable and extensible, allowing for the evaluation of multiple models across multiple object categories in a single run.

The pipeline handles everything from data preparation and anomaly map generation to metric calculation and results summarization.

## Features

- **End-to-End Automation**: A single command executes the entire pipeline, from data setup to the final report.
- **Multi-Model & Multi-Category**: Easily configure the pipeline to run evaluations for a list of different models across any subset of the 15 MVTec AD categories.
- **Configurable**: A central `pipeline_config.json` file allows you to define the scope of the evaluation runs without modifying the source code.
- **Clear Summaries**: The pipeline outputs final performance metrics (AU-PRO and AU-ROC) in clean, formatted tables for easy comparison.
- **Extensible**: Designed to be easily extended with new models.

## File Structure & Purpose

Here is a breakdown of the key files and directories in this evaluation pipeline:

| File / Directory               | Purpose                                                                                                                                                                                            |
| ------------------------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **`run_pipeline.py`**          | **Main entry point for the entire pipeline.** This script reads the configuration and orchestrates the execution of all other scripts in the correct order.                                         |
| **`pipeline_config.json`**     | **Central configuration file.** Here you define which models and which MVTec categories you want to evaluate. You also specify the paths for datasets and results.                                   |
| **`run_generation.py`**        | A helper script responsible for generating anomaly maps. It takes a model name and a category as input and saves the resulting anomaly maps in the appropriate directory structure.                  |
| `evaluate_multiple_experiments.py` | (Existing) An evaluation script that reads a configuration of multiple experiment results and calls `evaluate_experiment.py` for each one.                                                     |
| `evaluate_experiment.py`       | (Existing) The core evaluation script. It calculates AU-PRO and AU-ROC metrics for a single model's anomaly maps against the ground truth.                                                          |
| `print_metrics.py`             | (Existing) A utility that takes a folder of metric files and prints a formatted summary table to the console, comparing the performance of all evaluated models.                                     |
| `generic_util.py`              | (Existing) Contains various helper functions, including the canonical list of all 15 MVTec AD object names (`OBJECT_NAMES`).                                                                        |
| `mvtec_ad_structured/`         | A directory containing the MVTec AD dataset after it has been restructured by the pipeline into the format required for evaluation (i.e., separated by category, test/ground_truth, and defect type). |
| `generated_anomaly_maps/`      | The output directory where all anomaly maps generated by `run_generation.py` are stored, organized by model name and category.                                                                     |
| `evaluation_metrics/`          | The final output directory containing the detailed `metrics.json` file for each model and the overall summary tables.                                                                                  |

## How to Use

### 1. Installation

Ensure you have installed all necessary dependencies. This project uses `uv` for environment and package management.

```bash
# Install dependencies
uv pip install -r requirements.txt
```

### 2. Add Your Model

To evaluate your own custom model, you need to:

A. **Update `run_generation.py`**: Open the `run_generation.py` script and add the logic to load your model in the `get_model()` function.

   ```python
   # In run_generation.py
   def get_model(model_name, image_size):
       """Factory function to get a model instance by name."""
       if model_name == "DummyModel":
           return DummyModel(image_size=image_size)
       # ADD YOUR MODEL LOADING LOGIC HERE
       elif model_name == "MyPatchCoreModel":
           my_model = ... # Your model loading code
           return my_model
       else:
           raise ValueError(f"Unknown model name: {model_name}")
   ```

B. **Update Configuration**: Add the name of your model to the `"models"` list in `pipeline_config.json`.

   ```json
   {
     "models": ["DummyModel", "MyPatchCoreModel"],
     "categories": ["bottle", "cable", ...],
     ...
   }
   ```

### 3. Configure the Evaluation

Edit `pipeline_config.json` to specify which models and which of the 15 MVTec AD categories you wish to evaluate. By default, it is configured to run the `DummyModel` on all 15 categories.

### 4. Run the Pipeline

Execute the main pipeline script. It will run the complete end-to-end process.

```bash
uv run run_pipeline.py
```

### 5. Check the Results

The final performance summary tables will be printed to your console. Detailed JSON files for each model's performance are saved in the `evaluation_metrics/` directory.

## The Evaluation Process in Detail

When you execute `uv run run_pipeline.py`, the following steps occur:

1.  **Configuration Load**: The script reads `pipeline_config.json` to get the list of models and categories for the run.
2.  **Directory Setup**: It cleans and prepares the `generated_anomaly_maps/` and `evaluation_metrics/` directories.
3.  **Dataset Restructuring**: It checks for the existence of the `mvtec_ad_structured/` directory. If not found, it downloads the MVTec AD dataset via `fiftyone` and restructures it into the required format for evaluation. This is a one-time setup.
4.  **Anomaly Map Generation**: The script iterates through every model and category combination defined in the config. For each pair, it calls `run_generation.py` as a subprocess to generate and save the corresponding anomaly maps.
5.  **Evaluation Config Creation**: Once all maps are generated, the pipeline creates a temporary `eval_config.json` file. This file tells the next script where to find the anomaly maps for each model.
6.  **Metric Calculation**: The script calls `evaluate_multiple_experiments.py`, which reads `eval_config.json` and, in turn, calls `evaluate_experiment.py` to compute the AU-PRO and AU-ROC metrics for every model across all categories. The results are saved as JSON files in the `evaluation_metrics/` directory.
7.  **Result Summarization**: Finally, the script calls `print_metrics.py`, which reads the metric files and prints the final, formatted comparison tables to the console.
